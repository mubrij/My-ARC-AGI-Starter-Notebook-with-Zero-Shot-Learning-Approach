{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJqE28neX0NA"
      },
      "source": [
        "# ARC-AGI Starter Notebook\n",
        "\n",
        "This notebook provides a baseline solution for the ARC-AGI competition using a zero-shot approach with Qwen2.5-7B-Instruct. It loads the test dataset, processes grids, generates predictions, and creates submission files. Each code cell is explained to help beginners understand the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lJMjUuhX5ok"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "We import necessary libraries for data processing, machine learning, and logging:\n",
        "- `json` and `csv` for handling input/output files.\n",
        "- `numpy` for grid manipulation.\n",
        "- `transformers` for loading DistilGPT2 model and tokenizer.\n",
        "- `torch` for model inference.\n",
        "- `logging` for tracking errors and progress.\n",
        "- `re` for text parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoPUSQ4gXj5-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import csv\n",
        "import logging\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sn0ravvXj5_"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "These functions convert grids to text and back for model input/output:\n",
        "- `grid_to_text`: Converts a 2D grid of integers to a string, with each row as a continuous sequence of digits.\n",
        "- `text_to_grid`: Parses model-generated text back to a 2D grid, handling potential errors and cleaning up text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1nGvmGkXj6A"
      },
      "outputs": [],
      "source": [
        "def grid_to_text(grid: List[List[int]]) -> str:\n",
        "    \"\"\"Convert a numeric grid to text representation.\"\"\"\n",
        "    lines = []\n",
        "    for row in grid:\n",
        "        row_str = ''.join(str(cell) for cell in row)  # Continuous digits\n",
        "        lines.append(row_str)\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "def text_to_grid(text: str) -> List[List[int]]:\n",
        "    \"\"\"Convert text back to numeric grid with improved parsing.\"\"\"\n",
        "    try:\n",
        "        text = text.strip()\n",
        "        text = re.sub(r'^(Output grid:|Predicted output:|Answer:|Result:)', '', text, flags=re.IGNORECASE)\n",
        "        text = text.strip()\n",
        "        lines = text.split('\\n')\n",
        "        grid = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            numbers = []\n",
        "            for char in line:\n",
        "                if char.isdigit():\n",
        "                    numbers.append(int(char))\n",
        "            if numbers:\n",
        "                grid.append(numbers)\n",
        "        if grid and all(len(row) == len(grid[0]) for row in grid):\n",
        "            return grid\n",
        "        return [[0]]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error parsing grid: {e}\")\n",
        "        return [[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrZyQO8_Xj6B"
      },
      "source": [
        "## Visualization Function\n",
        "\n",
        "This function visualizes input and predicted grids side by side using Matplotlib. It helps understand the dataset and model predictions.\n",
        "- Uses `tab10` colormap to map integers 0–9 to colors.\n",
        "- Displays input and output/predicted grids for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciVHTr4_Xj6C"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_grid_pair(input_grid, output_grid, title=\"Grid Pair\"):\n",
        "    \"\"\"Plot input and output grids side by side.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax1.imshow(np.array(input_grid), cmap='tab10', vmin=0, vmax=9)\n",
        "    ax1.set_title(\"Input Grid\")\n",
        "    ax1.axis('off')\n",
        "    ax2.imshow(np.array(output_grid), cmap='tab10', vmin=0, vmax=9)\n",
        "    ax2.set_title(\"Output/Predicted Grid\")\n",
        "    ax2.axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVwcFEY3Xj6C"
      },
      "source": [
        "## ARCSolver Class\n",
        "\n",
        "This class encapsulates the zero-shot solver using Qwen2.5-7B-Instruct:\n",
        "- Initializes the model and tokenizer, with GPU support if available.\n",
        "- `solve`: Processes a single test case, generating a prediction or falling back to copying the input.\n",
        "- `predict_with_fallback`: Handles multiple test cases per problem.\n",
        "- `pad_grid`: Adjusts grid size to match required rows.\n",
        "- `is_valid_grid`: Validates predicted grids to ensure reasonable size and values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnTdACniXj6D"
      },
      "outputs": [],
      "source": [
        "class ARCSolver:\n",
        "    def __init__(self, row_counts: Dict[str, int], model_name: str = \"Qwen/Qwen2.5-7B-Instruct\"):\n",
        "        \"\"\"\n",
        "        Initialize solver with Qwen model for enhanced zero-shot reasoning.\n",
        "\n",
        "        Args:\n",
        "            row_counts: Dictionary mapping problem_id to required rows\n",
        "            model_name: Hugging Face model name (default: Qwen2.5-7B-Instruct)\n",
        "        \"\"\"\n",
        "        self.row_counts = row_counts\n",
        "        self.model_name = model_name\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Initialize model and tokenizer\n",
        "        self._load_model()\n",
        "\n",
        "        # Generation configuration for better outputs\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.1,  # Low temperature for more deterministic outputs\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.tokenizer.eos_token_id if self.tokenizer else None,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load the specified model and tokenizer.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading {self.model_name}...\")\n",
        "\n",
        "            # Load tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_name,\n",
        "                trust_remote_code=True,\n",
        "                padding_side=\"left\"\n",
        "            )\n",
        "\n",
        "            # Set pad token if not exists\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Load model with appropriate settings\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            self.model.eval()\n",
        "            logger.info(f\"Successfully loaded {self.model_name} on {self.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load {self.model_name}: {e}\")\n",
        "            logger.info(\"Falling back to DistilGPT2...\")\n",
        "            self._load_fallback_model()\n",
        "\n",
        "    def _load_fallback_model(self):\n",
        "        \"\"\"Load fallback model if main model fails.\"\"\"\n",
        "        try:\n",
        "            self.model_name = \"distilgpt2\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "            logger.info(\"Fallback model DistilGPT2 loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load fallback model: {e}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def grid_to_text(self, grid: List[List[int]]) -> str:\n",
        "        \"\"\"Convert grid to readable text format.\"\"\"\n",
        "        return '\\n'.join([''.join(map(str, row)) for row in grid])\n",
        "\n",
        "    def text_to_grid(self, text: str) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Convert text back to grid format with robust parsing.\n",
        "\n",
        "        Args:\n",
        "            text: Generated text containing grid\n",
        "\n",
        "        Returns:\n",
        "            Parsed grid as List[List[int]]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            lines = text.strip().split('\\n')\n",
        "            grid = []\n",
        "\n",
        "            for line in lines:\n",
        "                # Clean the line - remove non-digit characters except spaces\n",
        "                cleaned = re.sub(r'[^0-9\\s]', '', line.strip())\n",
        "                if not cleaned:\n",
        "                    continue\n",
        "\n",
        "                # Convert to digits\n",
        "                if ' ' in cleaned:\n",
        "                    # Space-separated format\n",
        "                    row = [int(x) for x in cleaned.split() if x.isdigit()]\n",
        "                else:\n",
        "                    # Concatenated format\n",
        "                    row = [int(x) for x in cleaned if x.isdigit()]\n",
        "\n",
        "                if row:  # Only add non-empty rows\n",
        "                    grid.append(row)\n",
        "\n",
        "            return grid if grid else [[0]]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to parse grid from text: {e}\")\n",
        "            return [[0]]\n",
        "\n",
        "    def create_enhanced_prompt(self, input_grid: List[List[int]], target_rows: int,\n",
        "                             training_examples: List = None) -> str:\n",
        "        \"\"\"\n",
        "        Create an enhanced prompt for better ARC reasoning.\n",
        "\n",
        "        Args:\n",
        "            input_grid: Input grid for the problem\n",
        "            target_rows: Expected number of rows in output\n",
        "            training_examples: Optional training examples for few-shot learning\n",
        "\n",
        "        Returns:\n",
        "            Formatted prompt string\n",
        "        \"\"\"\n",
        "        input_text = self.grid_to_text(input_grid)\n",
        "        rows, cols = len(input_grid), len(input_grid[0])\n",
        "\n",
        "        prompt = f\"\"\"You are an expert at solving ARC (Abstraction and Reasoning Corpus) puzzles. These puzzles require identifying patterns and transformations in grids of colored cells (represented by digits 0-9).\n",
        "\n",
        "TASK: Analyze the input grid and predict the output grid by identifying the underlying transformation pattern.\n",
        "\n",
        "INPUT GRID ({rows}x{cols}):\n",
        "{input_text}\n",
        "\n",
        "ANALYSIS STEPS:\n",
        "1. Identify unique values and their positions\n",
        "2. Look for geometric patterns (lines, shapes, symmetries)\n",
        "3. Check for directional transformations\n",
        "4. Consider boundary effects and edge cases\n",
        "5. Apply the transformation rule consistently\n",
        "\n",
        "OUTPUT REQUIREMENTS:\n",
        "- Generate exactly {target_rows} rows\n",
        "- Each row should have {cols} digits (0-9)\n",
        "- Maintain consistent column width\n",
        "- Apply identified pattern systematically\n",
        "\n",
        "OUTPUT GRID:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_prediction(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate prediction using the loaded model.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input prompt for the model\n",
        "\n",
        "        Returns:\n",
        "            Generated text response\n",
        "        \"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=2048\n",
        "            )\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    generation_config=self.generation_config\n",
        "                )\n",
        "\n",
        "            # Decode response\n",
        "            generated_text = self.tokenizer.decode(\n",
        "                outputs[0],\n",
        "                skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # Extract only the new content (after prompt)\n",
        "            response = generated_text[len(prompt):].strip()\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Generation failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def solve(self, problem: Dict, problem_id: str) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Solve a single test case using enhanced reasoning.\n",
        "\n",
        "        Args:\n",
        "            problem: Problem dictionary containing test cases\n",
        "            problem_id: Unique identifier for the problem\n",
        "\n",
        "        Returns:\n",
        "            Predicted output grid\n",
        "        \"\"\"\n",
        "        try:\n",
        "            input_grid = problem['test'][0]['input']\n",
        "            target_rows = self.row_counts.get(problem_id, len(input_grid))\n",
        "\n",
        "            # Validate input\n",
        "            if not self.is_valid_grid(input_grid):\n",
        "                logger.warning(f\"Invalid input grid for {problem_id}\")\n",
        "                return self.create_fallback_grid(input_grid, target_rows)\n",
        "\n",
        "            # Create enhanced prompt\n",
        "            prompt = self.create_enhanced_prompt(input_grid, target_rows)\n",
        "\n",
        "            # Generate prediction\n",
        "            response = self.generate_prediction(prompt)\n",
        "\n",
        "            if response:\n",
        "                # Parse generated grid\n",
        "                predicted_grid = self.text_to_grid(response)\n",
        "\n",
        "                # Validate and adjust prediction\n",
        "                if self.is_valid_grid(predicted_grid):\n",
        "                    adjusted_grid = self.adjust_grid_size(predicted_grid, target_rows, len(input_grid[0]))\n",
        "                    if self.is_reasonable_prediction(adjusted_grid, input_grid):\n",
        "                        return adjusted_grid\n",
        "\n",
        "            # Fallback to input-based prediction\n",
        "            logger.info(f\"Using fallback for {problem_id}\")\n",
        "            return self.create_fallback_grid(input_grid, target_rows)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Solve failed for {problem_id}: {e}\")\n",
        "            return self.create_fallback_grid(problem['test'][0]['input'],\n",
        "                                           self.row_counts.get(problem_id, 11))\n",
        "\n",
        "    def predict_with_fallback(self, problem: Dict, problem_id: str) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Predict outputs for all test cases with enhanced fallback logic.\n",
        "\n",
        "        Args:\n",
        "            problem: Problem dictionary\n",
        "            problem_id: Problem identifier\n",
        "\n",
        "        Returns:\n",
        "            List of predicted grids for each test case\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        target_rows = self.row_counts.get(problem_id, 11)\n",
        "\n",
        "        for i, test_case in enumerate(problem['test']):\n",
        "            logger.info(f\"Processing test case {i+1}/{len(problem['test'])} for {problem_id}\")\n",
        "\n",
        "            try:\n",
        "                input_grid = test_case['input']\n",
        "\n",
        "                # Create problem dict for single test case\n",
        "                single_problem = {'test': [test_case]}\n",
        "                prediction = self.solve(single_problem, problem_id)\n",
        "                predictions.append(prediction)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to process test case {i+1} for {problem_id}: {e}\")\n",
        "                fallback = self.create_fallback_grid(test_case['input'], target_rows)\n",
        "                predictions.append(fallback)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def adjust_grid_size(self, grid: List[List[int]], target_rows: int, target_cols: int) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Adjust grid to match target dimensions.\n",
        "\n",
        "        Args:\n",
        "            grid: Input grid to adjust\n",
        "            target_rows: Desired number of rows\n",
        "            target_cols: Desired number of columns\n",
        "\n",
        "        Returns:\n",
        "            Adjusted grid\n",
        "        \"\"\"\n",
        "        if not grid:\n",
        "            return [[0] * target_cols for _ in range(target_rows)]\n",
        "\n",
        "        # Adjust rows\n",
        "        if len(grid) > target_rows:\n",
        "            grid = grid[:target_rows]\n",
        "        elif len(grid) < target_rows:\n",
        "            last_row = grid[-1] if grid else [0] * target_cols\n",
        "            while len(grid) < target_rows:\n",
        "                grid.append(last_row[:])\n",
        "\n",
        "        # Adjust columns\n",
        "        for i, row in enumerate(grid):\n",
        "            if len(row) > target_cols:\n",
        "                grid[i] = row[:target_cols]\n",
        "            elif len(row) < target_cols:\n",
        "                grid[i].extend([0] * (target_cols - len(row)))\n",
        "\n",
        "        return grid\n",
        "\n",
        "    def create_fallback_grid(self, input_grid: List[List[int]], target_rows: int) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Create a reasonable fallback grid based on input.\n",
        "\n",
        "        Args:\n",
        "            input_grid: Original input grid\n",
        "            target_rows: Target number of rows\n",
        "\n",
        "        Returns:\n",
        "            Fallback grid\n",
        "        \"\"\"\n",
        "        if not input_grid:\n",
        "            return [[0] for _ in range(target_rows)]\n",
        "\n",
        "        cols = len(input_grid[0])\n",
        "\n",
        "        # Simple strategy: copy input and pad/truncate as needed\n",
        "        if len(input_grid) >= target_rows:\n",
        "            return input_grid[:target_rows]\n",
        "        else:\n",
        "            fallback = input_grid[:]\n",
        "            # Pad with last row or zeros\n",
        "            pad_row = input_grid[-1][:] if input_grid else [0] * cols\n",
        "            while len(fallback) < target_rows:\n",
        "                fallback.append(pad_row[:])\n",
        "            return fallback\n",
        "\n",
        "    def is_valid_grid(self, grid: List[List[int]]) -> bool:\n",
        "        \"\"\"\n",
        "        Validate if grid format is correct.\n",
        "\n",
        "        Args:\n",
        "            grid: Grid to validate\n",
        "\n",
        "        Returns:\n",
        "            True if valid, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not grid or not grid[0]:\n",
        "                return False\n",
        "\n",
        "            # Check rectangular shape\n",
        "            col_count = len(grid[0])\n",
        "            if not all(len(row) == col_count for row in grid):\n",
        "                return False\n",
        "\n",
        "            # Check value range\n",
        "            for row in grid:\n",
        "                for val in row:\n",
        "                    if not isinstance(val, int) or val < 0 or val > 9:\n",
        "                        return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def is_reasonable_prediction(self, predicted_grid: List[List[int]],\n",
        "                               input_grid: List[List[int]]) -> bool:\n",
        "        \"\"\"\n",
        "        Check if prediction is reasonable compared to input.\n",
        "\n",
        "        Args:\n",
        "            predicted_grid: Predicted output grid\n",
        "            input_grid: Original input grid\n",
        "\n",
        "        Returns:\n",
        "            True if reasonable, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Basic validation\n",
        "            if not self.is_valid_grid(predicted_grid):\n",
        "                return False\n",
        "\n",
        "            # Size reasonableness check\n",
        "            pred_size = len(predicted_grid) * len(predicted_grid[0])\n",
        "            input_size = len(input_grid) * len(input_grid[0])\n",
        "\n",
        "            # Allow up to 4x larger or 4x smaller\n",
        "            if pred_size > input_size * 4 or pred_size < max(1, input_size // 4):\n",
        "                return False\n",
        "\n",
        "            # Check for some variation (not all same values)\n",
        "            flat_pred = [val for row in predicted_grid for val in row]\n",
        "            if len(set(flat_pred)) == 1 and len(flat_pred) > 4:\n",
        "                # All same values might be suspicious for larger grids\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOC1UdmXj6E"
      },
      "source": [
        "## Load Test Data\n",
        "\n",
        "Loads the test dataset from `test (1).json` into a dictionary for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kbJ5XCyXj6E"
      },
      "outputs": [],
      "source": [
        "def load_test_data(test_path: str) -> Dict:\n",
        "    \"\"\"Load test JSON data.\"\"\"\n",
        "    with open(test_path, 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "    return test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu8_9cGaXj6F"
      },
      "source": [
        "## Parse Sample Submission\n",
        "\n",
        "Extracts the required number of rows for each problem from the sample submission CSV (`SampleSubmission (29).csv`). This ensures predictions match the expected output size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqcN890MXj6F"
      },
      "outputs": [],
      "source": [
        "def get_row_counts(sample_csv_path: str) -> Dict[str, int]:\n",
        "    \"\"\"Parse sample submission CSV to determine required rows per problem.\"\"\"\n",
        "    row_counts = {}\n",
        "    with open(sample_csv_path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            problem_id = '_'.join(row['ID'].split('_')[:2])\n",
        "            row_counts[problem_id] = max(row_counts.get(problem_id, 0), int(row['ID'].split('_')[-1]))\n",
        "    return row_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bODwTShVXj6F"
      },
      "source": [
        "## Create Submission\n",
        "\n",
        "Generates submission files in JSON and CSV formats:\n",
        "- Iterates through test problems, predicting outputs for each test case.\n",
        "- Handles multiple test cases per problem and formats IDs correctly.\n",
        "- Includes error handling and fallback to input copying.\n",
        "- Visualizes the first test case’s input and prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQpa5PTCXj6G"
      },
      "outputs": [],
      "source": [
        "def create_submission(test_data: Dict, model: ARCSolver,\n",
        "                             output_file: str = 'submission.json',\n",
        "                             csv_file: str = 'submission.csv') -> Tuple[Dict, List]:\n",
        "    \"\"\"\n",
        "    Create submission files with enhanced error handling and progress tracking.\n",
        "\n",
        "    Args:\n",
        "        test_data: Dictionary of test problems\n",
        "        model: Trained ARC solver model\n",
        "        output_file: JSON output filename\n",
        "        csv_file: CSV output filename\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (submission_json, submission_csv)\n",
        "    \"\"\"\n",
        "    submission_json = {}\n",
        "    submission_csv = []\n",
        "    total_problems = len(test_data)\n",
        "    successful_predictions = 0\n",
        "\n",
        "    print(f\"\\n Generating submissions for {total_problems} test problems...\")\n",
        "    print(f\" Using model: {model.model_name}\")\n",
        "    print(f\" Device: {model.device}\")\n",
        "\n",
        "    for i, (problem_id, problem) in enumerate(test_data.items()):\n",
        "        # Progress tracking\n",
        "        if (i + 1) % 5 == 0 or i == 0:\n",
        "            print(f\" Progress: {i+1}/{total_problems} ({(i+1)/total_problems*100:.1f}%)\")\n",
        "\n",
        "        try:\n",
        "            num_test_cases = len(problem['test'])\n",
        "            logger.info(f\"Processing {problem_id} with {num_test_cases} test case(s)\")\n",
        "\n",
        "            # Generate predictions using enhanced model\n",
        "            problem_for_prediction = {\n",
        "                'train': problem.get('train', []),  # Include training if available\n",
        "                'test': problem['test']\n",
        "            }\n",
        "\n",
        "            predictions = model.predict_with_fallback(problem_for_prediction, problem_id)\n",
        "\n",
        "            # Validate predictions count\n",
        "            if len(predictions) != num_test_cases:\n",
        "                logger.warning(f\"Prediction count mismatch for {problem_id}: \"\n",
        "                             f\"expected {num_test_cases}, got {len(predictions)}\")\n",
        "                # Ensure correct number of predictions\n",
        "                while len(predictions) < num_test_cases:\n",
        "                    target_rows = model.row_counts.get(problem_id, 11)\n",
        "                    fallback = model.create_fallback_grid(problem['test'][0]['input'], target_rows)\n",
        "                    predictions.append(fallback)\n",
        "                predictions = predictions[:num_test_cases]\n",
        "\n",
        "            submission_json[problem_id] = predictions\n",
        "            successful_predictions += 1\n",
        "\n",
        "            # Generate CSV entries\n",
        "            for matrix_idx, prediction in enumerate(predictions):\n",
        "                matrix_num = matrix_idx + 1\n",
        "                for row_idx, row in enumerate(prediction):\n",
        "                    row_num = row_idx + 1\n",
        "                    row_string = ''.join(str(cell) for cell in row)\n",
        "\n",
        "                    # Create ID based on number of test cases\n",
        "                    if num_test_cases > 1:\n",
        "                        csv_id = f\"{problem_id}_{matrix_num}_{row_num}\"\n",
        "                    else:\n",
        "                        csv_id = f\"{problem_id}_{row_num}\"\n",
        "\n",
        "                    submission_csv.append({\n",
        "                        'ID': csv_id,\n",
        "                        'row': row_string\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {problem_id}: {e}\")\n",
        "\n",
        "            # Enhanced fallback handling\n",
        "            target_rows = model.row_counts.get(problem_id, 11)\n",
        "            fallback_predictions = []\n",
        "\n",
        "            for test_case in problem['test']:\n",
        "                fallback_grid = model.create_fallback_grid(test_case['input'], target_rows)\n",
        "                fallback_predictions.append(fallback_grid)\n",
        "\n",
        "            submission_json[problem_id] = fallback_predictions\n",
        "\n",
        "            # Generate CSV for fallback\n",
        "            for matrix_idx, fallback_grid in enumerate(fallback_predictions):\n",
        "                matrix_num = matrix_idx + 1\n",
        "                for row_idx, row in enumerate(fallback_grid):\n",
        "                    row_num = row_idx + 1\n",
        "                    row_string = ''.join(str(cell) for cell in row)\n",
        "\n",
        "                    if len(fallback_predictions) > 1:\n",
        "                        csv_id = f\"{problem_id}_{matrix_num}_{row_num}\"\n",
        "                    else:\n",
        "                        csv_id = f\"{problem_id}_{row_num}\"\n",
        "\n",
        "                    submission_csv.append({\n",
        "                        'ID': csv_id,\n",
        "                        'row': row_string\n",
        "                    })\n",
        "\n",
        "    # Save files\n",
        "    print(f\"\\n Saving submission files...\")\n",
        "\n",
        "    # Save JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(submission_json, f, separators=(',', ':'))\n",
        "\n",
        "    # Save CSV\n",
        "    with open(csv_file, 'w', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['ID', 'row'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(submission_csv)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\n Submission Generation Complete!\")\n",
        "    print(f\" Summary:\")\n",
        "    print(f\"   JSON format: {output_file}\")\n",
        "    print(f\"   CSV format: {csv_file}\")\n",
        "    print(f\"   Total problems: {len(submission_json)}\")\n",
        "    print(f\"   Successful predictions: {successful_predictions}\")\n",
        "    print(f\"   Total CSV rows: {len(submission_csv)}\")\n",
        "    print(f\"   Success rate: {successful_predictions/total_problems*100:.1f}%\")\n",
        "\n",
        "    # Show sample entries\n",
        "    if submission_csv:\n",
        "        print(f\"\\n Sample CSV entries (first 5 rows):\")\n",
        "        for i in range(min(5, len(submission_csv))):\n",
        "            entry = submission_csv[i]\n",
        "            print(f\"   {entry['ID']}: {entry['row'][:50]}{'...' if len(entry['row']) > 50 else ''}\")\n",
        "\n",
        "    # Multi-test case analysis\n",
        "    multi_test_problems = [(p_id, p) for p_id, p in test_data.items() if len(p['test']) > 1]\n",
        "    if multi_test_problems:\n",
        "        print(f\"\\n Multi-test case problems: {len(multi_test_problems)}\")\n",
        "        example_id, example_prob = multi_test_problems[0]\n",
        "        print(f\"   Example '{example_id}': {len(example_prob['test'])} test cases\")\n",
        "\n",
        "        if example_id in submission_json:\n",
        "            for i, pred in enumerate(submission_json[example_id]):\n",
        "                print(f\"   Matrix {i+1}: {len(pred)}×{len(pred[0]) if pred else 0}\")\n",
        "\n",
        "    return submission_json, submission_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTkfWKPyXj6G"
      },
      "source": [
        "## Main Execution\n",
        "\n",
        "Executes the pipeline:\n",
        "- Loads test data and row counts.\n",
        "- Initializes the solver and generates submissions.\n",
        "- Handles errors and logs progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a87f968382b143e682139361dcbf5b2c",
            "ac59a3eb2ad44f71a141736d5723c054",
            "79a4510a69924685bbd5ebc6fe782be9",
            "72828b17f17244ffa6c76a14afe171ae",
            "7f8bd94da89f458f85e0e1dc039ffee2",
            "34efb34417ba44ffaa8cca6d80d33fad",
            "61f814de5cee4fa5a8c6c297e65b4d71",
            "8e1907176fe147fdb8f7b029363e93f1",
            "5b18b8db15344fa19c60e7ba649e12c3",
            "29d44a3599d3402dba69b8742440a9bb",
            "28bdb57de3ed41acb786228bd78edf32",
            "a93ec9688bbf420693263ad27ecde900",
            "5edb236b58ab4c8f93d6c013d595d5b5",
            "0fa4bc9056304c4b8f97350f3c3144e7",
            "1458b681b1284986a8727420926823ab",
            "c01ab24c449d4e0c801954e892266f1d",
            "defad69caff748bcbbd745726c2fc687",
            "c6132d51caff43cb8bbb615c54aaaf48",
            "1144dea7d3d448798127b0a352ccdb13",
            "00cbca82dde448d68e35793d2bccf79c",
            "9db24f22138c46dd81f3cd3c0ec18270",
            "d15419156bc441aa962b0362f0e84802"
          ]
        },
        "id": "wB5hAmxaXj6H",
        "outputId": "964a6ed4-ebda-400d-dac6-f1dbf021be4b"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_path = \"/content/test (2).json\"\n",
        "sample_csv_path = \"/content/SampleSubmission (30).csv\"\n",
        "output_json = \"/content/submission.json\"\n",
        "output_csv = \"/content/submission.csv\"\n",
        "\n",
        "try:\n",
        "    test_data = load_test_data(test_path)\n",
        "    logger.info(f\"Loaded {len(test_data)} test problems\")\n",
        "\n",
        "    row_counts = get_row_counts(sample_csv_path)\n",
        "    logger.info(f\"Determined row counts for {len(row_counts)} problems\")\n",
        "\n",
        "    solver = ARCSolver(\n",
        "        row_counts=row_counts,\n",
        "        model_name=\"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "    )\n",
        "    submission_json, submission_csv = create_submission(test_data, solver, output_json, output_csv)\n",
        "\n",
        "    print(f\"\\n Submission generation completed successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Submission generation failed: {e}\")\n",
        "    logger.error(f\"Submission generation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZBrSI9nXj6H"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Run the notebook to generate `submission.json` and `submission.csv`.\n",
        "- Visualize more test cases by modifying the `plot_grid_pair` call in the `create_submission` function.\n",
        "- Experiment with different prompts or models to improve predictions.\n",
        "- Share your results or questions on the Zindi leaderboard discussion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3S4lFdHCYj8Q",
        "outputId": "d1d210e5-cc01-4559-cdde-c82fcb09b332"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.read_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "l7wvWy0JhLYw",
        "outputId": "83114cdd-5801-4002-96c0-5ff8871f2778"
      },
      "outputs": [],
      "source": [
        "pd.read_csv(\"/content/SampleSubmission (30).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUwO9DhphW9i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00cbca82dde448d68e35793d2bccf79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fa4bc9056304c4b8f97350f3c3144e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1144dea7d3d448798127b0a352ccdb13",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00cbca82dde448d68e35793d2bccf79c",
            "value": 242
          }
        },
        "1144dea7d3d448798127b0a352ccdb13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1458b681b1284986a8727420926823ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9db24f22138c46dd81f3cd3c0ec18270",
            "placeholder": "​",
            "style": "IPY_MODEL_d15419156bc441aa962b0362f0e84802",
            "value": " 242/242 [00:00&lt;00:00, 25.9kB/s]"
          }
        },
        "28bdb57de3ed41acb786228bd78edf32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29d44a3599d3402dba69b8742440a9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34efb34417ba44ffaa8cca6d80d33fad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b18b8db15344fa19c60e7ba649e12c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5edb236b58ab4c8f93d6c013d595d5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_defad69caff748bcbbd745726c2fc687",
            "placeholder": "​",
            "style": "IPY_MODEL_c6132d51caff43cb8bbb615c54aaaf48",
            "value": "generation_config.json: 100%"
          }
        },
        "61f814de5cee4fa5a8c6c297e65b4d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72828b17f17244ffa6c76a14afe171ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29d44a3599d3402dba69b8742440a9bb",
            "placeholder": "​",
            "style": "IPY_MODEL_28bdb57de3ed41acb786228bd78edf32",
            "value": " 3.09G/3.09G [01:35&lt;00:00, 115MB/s]"
          }
        },
        "79a4510a69924685bbd5ebc6fe782be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e1907176fe147fdb8f7b029363e93f1",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b18b8db15344fa19c60e7ba649e12c3",
            "value": 3087467144
          }
        },
        "7f8bd94da89f458f85e0e1dc039ffee2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1907176fe147fdb8f7b029363e93f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db24f22138c46dd81f3cd3c0ec18270": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a87f968382b143e682139361dcbf5b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac59a3eb2ad44f71a141736d5723c054",
              "IPY_MODEL_79a4510a69924685bbd5ebc6fe782be9",
              "IPY_MODEL_72828b17f17244ffa6c76a14afe171ae"
            ],
            "layout": "IPY_MODEL_7f8bd94da89f458f85e0e1dc039ffee2"
          }
        },
        "a93ec9688bbf420693263ad27ecde900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5edb236b58ab4c8f93d6c013d595d5b5",
              "IPY_MODEL_0fa4bc9056304c4b8f97350f3c3144e7",
              "IPY_MODEL_1458b681b1284986a8727420926823ab"
            ],
            "layout": "IPY_MODEL_c01ab24c449d4e0c801954e892266f1d"
          }
        },
        "ac59a3eb2ad44f71a141736d5723c054": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34efb34417ba44ffaa8cca6d80d33fad",
            "placeholder": "​",
            "style": "IPY_MODEL_61f814de5cee4fa5a8c6c297e65b4d71",
            "value": "model.safetensors: 100%"
          }
        },
        "c01ab24c449d4e0c801954e892266f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6132d51caff43cb8bbb615c54aaaf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15419156bc441aa962b0362f0e84802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "defad69caff748bcbbd745726c2fc687": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
